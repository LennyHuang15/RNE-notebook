{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import signal\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import os, time, json\n",
    "from math import asin\n",
    "from random import choice, choices\n",
    "from shutil import copyfile\n",
    "\n",
    "def loadArrayInt(fn, usecols=None):\n",
    "    return np.array(pd.read_csv(fn, sep=' ', header=None, usecols=usecols, dtype=np.int32))\n",
    "def save_csv(fn, data):\n",
    "    pd.DataFrame(data).to_csv(fn, sep=' ', header=0, index=False)\n",
    "\n",
    "def read_node(fn):\n",
    "    with open(fn, 'r') as fp:\n",
    "        n_node, n_edge = [int(x) for x in fp.readline()[:-1].split(' ')[:2]]\n",
    "    edges = np.array(pd.read_csv(fn, sep=' ', header=None, skiprows=1, dtype=np.int32))\n",
    "    print(n_node, n_edge)\n",
    "    assert(n_edge == edges.shape[0])\n",
    "    return edges, n_node\n",
    "def partDict(parts_):\n",
    "    part_dict, cnt = {}, 0\n",
    "    for part_ in parts_:\n",
    "        if(not part_ in part_dict):\n",
    "            part_dict[part_] = cnt\n",
    "            cnt += 1\n",
    "    parts_ = np.array([part_dict[part_] for part_ in parts_])\n",
    "#     print(\"get parts\", len(part_dict), np.max(parts_))\n",
    "    return parts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN, F_man_dim = \"BJ\", 64\n",
    "F_n_train, F_n_val = int(1e7), int(1e6)\n",
    "F_batch_size, F_learning_rate = 4096, 4e-2\n",
    "\n",
    "SaveEmb, n_part_left = 1, 8\n",
    "dir_road, dir_data = \"./data/\" + RN + '/', \"./train/\" + RN + '/'\n",
    "f_edge, = [dir_road+x for x in [RN+'.gr']]\n",
    "f_part_full = dir_road+\"Nodes_full_4_%d.data\"%n_part_left\n",
    "F_train_dir, f_log, f_stat = dir_data+\"model/\", dir_data+\"log.out\", dir_data+\"stat\"\n",
    "f_train, f_test = [dir_data + \"%s.data\"%x for x in ['train', 'test']]\n",
    "f_emb_output = dir_road + \"emb%d_\"%(F_man_dim)\n",
    "if(not os.path.exists(F_train_dir)):\n",
    "    os.mkdir(F_train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges_g, n_node = read_node(f_edge)\n",
    "nodes = np.zeros((n_node, 2))\n",
    "parts = loadArrayInt(f_part_full)[:, 1::2]\n",
    "print(\"nodes:\", nodes.shape, \"edges:\", edges_g.shape, \"parts:\", parts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for data normalization\n",
    "def toStd(data):\n",
    "    return (data - mean_train) / std_train\n",
    "def fromStd(data):\n",
    "    return data * std_train + mean_train\n",
    "\n",
    "# train, valid and infer\n",
    "def run_epoch(model, sess, idxs, y, istrain=True, bs=4096): # Training Process\n",
    "    loss, loss_step = [np.array([0.0]*3) for x in range(2)]\n",
    "    st, ed, times, step = 0, bs, 0, 2000000\n",
    "    time_step, times_old = step, 0\n",
    "    while st < idxs.shape[0] and ed <= idxs.shape[0]:\n",
    "        X_batch, y_batch = idxs[st:ed], y[st:ed]\n",
    "        feed = {model.x_: X_batch, model.y_: toStd(y_batch)}\n",
    "        if(istrain):\n",
    "            loss_, loss_abs_, loss_rel_, _ = sess.run([model.loss, model.loss_abs, model.loss_rel, model.train_op], feed)\n",
    "        else:\n",
    "            loss_, loss_abs_, loss_rel_ = sess.run([model.loss_val, model.loss_abs_val, model.loss_rel_val], feed)\n",
    "        loss += np.array([loss_, loss_abs_, loss_rel_])\n",
    "        st, ed, times = ed, ed+bs, times+1\n",
    "        if(times * bs >= time_step):\n",
    "            loss_step_, loss_step[:] = loss - loss_step, loss[:]\n",
    "            mean_step = np.mean(y[times_old*bs: times*bs])\n",
    "            num_, times_old = times - times_old, times\n",
    "            loss_step_ /= num_\n",
    "            print(\"(%d): %.5f,%.5f\"%(times * bs, loss_step_[1], loss_step_[2]))\n",
    "            time_step += step\n",
    "    return loss / times\n",
    "\n",
    "def inference(model, sess, idxs): # Test Process\n",
    "    start_time = time.time()\n",
    "    pred = sess.run(model.pred_val, {model.x_: idxs})\n",
    "    pred_ = fromStd(pred).astype(np.int)\n",
    "    during_time = time.time() - start_time\n",
    "    print(\"pred(%d)data: %d(mS)\"%(pred_.shape[0], int(during_time*1000)))\n",
    "    return pred_, np.mean(pred_)\n",
    "\n",
    "# save/restore model\n",
    "def save_emb(model, sess, fid):\n",
    "    np.save(F_train_dir+\"emb%08d\"%fid, mlp_model.embedding_i.eval())\n",
    "    print(\"model saved to \" + F_train_dir+\"emb%08d\"%fid)\n",
    "def load_emb(model, sess, fid):\n",
    "    sess.run(mlp_model.emb_ass, feed_dict={mlp_model.emb_new: np.load(F_train_dir+\"emb%08d.npy\"%fid)})\n",
    "def save_emb_int(model, fid, mid=0):\n",
    "    emb_ = np.load(F_train_dir+\"emb%08d.npy\"%fid)\n",
    "#     emb_ = mlp_model.embedding_i.eval()\n",
    "    emb_ = fromStd(emb_) / F_man_dim\n",
    "    save_csv(f_emb_output+str(mid), emb_.astype(np.int))\n",
    "    print(\"save to \", f_emb_output+str(mid))\n",
    "\n",
    "    \n",
    "# map training samples into hier level\n",
    "def getIdx(idxs, Transform=False, parts_=None):\n",
    "    if(Transform): return parts_[idxs];\n",
    "    else: return idxs;\n",
    "    \n",
    "def get_train_data(num, Transform, parts_, w):\n",
    "    global F_n_train, idxs_val, y_val, idxs_train, y_train\n",
    "    idx_ = np.random.choice(idxs_train.shape[0], idxs_train.shape[0], replace=False)\n",
    "    idxs_train, y_train = idxs_train[idx_], y_train[idx_]\n",
    "    idxs_train_, idxs_val_ = getIdx(idxs_train, Transform, parts_), getIdx(idxs_val, Transform, parts_)\n",
    "    print(\"#training data: %d\"%(y_train.shape[0]))\n",
    "    return idxs_train_, y_train, idxs_val_, y_val\n",
    "\n",
    "def train_epochs(sess, epochs, lr, bs, Transform=False, parts_=None, w=None, learning_rate_decay=True, checkpoint_id=0):\n",
    "    pre_losses, best_val_loss, stay_cnt, last_loss = [1e18] * 3, 1e18, 0, 1e18\n",
    "    lr_ass = mlp_model.learning_rate.assign(lr)\n",
    "    sess.run(lr_ass)\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        idxs_train, y_train, idxs_val, y_val = get_train_data(F_n_train, Transform, parts_, w)\n",
    "        \n",
    "        print(\"Epoch(%d): lr= %f, bs= %d\"%(epoch, mlp_model.learning_rate.eval(), bs))\n",
    "        print(\"train: (#samples): mean abs err, mean abs rel err\")\n",
    "        train_loss = run_epoch(mlp_model, sess, idxs_train, y_train, istrain=True, bs=bs)\n",
    "        print(\"valid:\")\n",
    "        val_loss = run_epoch(mlp_model, sess, idxs_val, y_val, istrain=False, bs=bs)\n",
    "\n",
    "        if val_loss[0] <= best_val_loss:  # when valid_accuracy > best_valid_accuracy, save the model\n",
    "            checkpoint_id += 1\n",
    "            save_emb(mlp_model, sess, checkpoint_id)\n",
    "            best_val_loss = val_loss[0]\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(\"Epoch \" + str(epoch) + \" of \" + str(epochs) + \" took \" + str(epoch_time) + \"s\")\n",
    "        print(\"  training loss:             %.5f,%.5f\"%tuple(train_loss[-2:]))\n",
    "        print(\"  validation loss:           %.5f,%.5f\"%tuple(val_loss[-2:]))\n",
    "        print(\"\")\n",
    "\n",
    "        if(train_loss[0] >= last_loss):\n",
    "            if(learning_rate_decay): stay_cnt += 1\n",
    "            if(stay_cnt >= 3): sess.run(mlp_model.learning_rate_decay_op); stay_cnt = 0; last_loss = train_loss[0]\n",
    "        else: stay_cnt = 0; last_loss = train_loss[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train = loadArrayInt(f_train)\n",
    "idxs_train, y_train = data_train[:, :2], data_train[:, 2]\n",
    "mean_train, std_train = np.mean(y_train), np.std(y_train)\n",
    "mean_train, std_train = 0, mean_train\n",
    "\n",
    "data_val = loadArrayInt(f_test)\n",
    "idxs_val, y_val = data_val[:, :2], data_val[:, 2]\n",
    "print(idxs_train.shape[0], idxs_val.shape[0], mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "class Model:\n",
    "    def __init__(self, learning_rate=0.04, learning_rate_decay_factor=0.8):\n",
    "        self.y_ = tf.placeholder(tf.float32, [None])\n",
    "        self.x_ = tf.placeholder(tf.int32, [None, 2])\n",
    "        self.initializer_uniform = tf.initializers.random_uniform(-3/2, 3/2)\n",
    "        \n",
    "        self.loss, self.pred, self.loss_abs, self.loss_rel = self.forward(True)\n",
    "        self.loss_val, self.pred_val, self.loss_abs_val, self.loss_rel_val = self.forward(False, reuse=True)\n",
    "\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=tf.float32)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)\n",
    "        self.emb_new = tf.placeholder(tf.float32, shape=self.embedding_i.shape)\n",
    "        self.emb_ass = self.embedding_i.assign(self.emb_new)\n",
    "\n",
    "        self.params = tf.trainable_variables()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)# update the BN params when training\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.RMSPropOptimizer(self.learning_rate, 0.9, 0.9)\n",
    "            gvs = optimizer.compute_gradients(self.loss)\n",
    "            self.train_op = optimizer.apply_gradients(gvs)\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2,\n",
    "                                    max_to_keep=5, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n",
    "        print(\"model inited\")\n",
    "                                    \n",
    "    def forward(self, is_train, reuse=None):\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            self.embedding_i = tf.get_variable(\"embedding_i\", [nodes.shape[0], F_man_dim], trainable=is_train, initializer=self.initializer_uniform)\n",
    "            xs_em = tf.nn.embedding_lookup(self.embedding_i, self.x_)#shape: (bs, 2, l_emb)\n",
    "            if(is_train): print(xs_em.shape)\n",
    "            dx_ = tf.abs(xs_em[:, 0, :] - xs_em[:, 1, :])# emb1 - emb2\n",
    "            pred = tf.reduce_mean(dx_, axis=1) #expect when w=1, pred and y_ are in the same level\n",
    "\n",
    "        if(is_train): print(pred.shape)\n",
    "        df_ = pred - self.y_\n",
    "        loss, loss_abs = tf.reduce_mean(tf.square(df_)), tf.reduce_mean(tf.abs(df_))\n",
    "        \n",
    "        y_, pred_ = fromStd(self.y_), fromStd(pred)\n",
    "        diff_abs_ = tf.abs(pred_ - y_)\n",
    "        diff_abs = tf.reduce_mean(diff_abs_)\n",
    "        diff_rel = tf.reduce_mean(diff_abs_ / tf.maximum(y_, 1.0))#(y_ + F_rel_fac))\n",
    "        return loss, pred, diff_abs, diff_rel\n",
    "\n",
    "mlp_model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check #nodes in level $lev\n",
    "lev = 8\n",
    "np.max(partDict(parts[:, lev])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch, n_hier, ptsIdxs = 10, 6, np.array([4, 5, 6, 7, 8, 11])\n",
    "epochs, bss, lrs = np.array([1, 1, 1, 1, 1, n_epoch]), np.array([F_batch_size]*n_hier), np.array([F_learning_rate]*n_hier)\n",
    "embeddings = np.zeros((nodes.shape[0], F_man_dim))\n",
    "F_train_version = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
    "    if(F_train_version == 0):\n",
    "        tf.global_variables_initializer().run()\n",
    "    else:\n",
    "        restore_model(mlp_model, sess, F_train_version)\n",
    "    for idx in range(0, n_hier):\n",
    "        parts_ = partDict(parts[:, ptsIdxs[idx]])\n",
    "        n_p = np.max(parts_) + 1\n",
    "        print(\"level %d: %d nodes\"%(idx, n_p))\n",
    "        # input embs to model\n",
    "        if(idx != 0):#assign new embs for next training(prolongation)\n",
    "            parts__ = partDict(parts[:, ptsIdxs[idx-1]])\n",
    "            embeddings[parts_] = mlp_model.embedding_i.eval()[parts__]\n",
    "            sess.run(mlp_model.emb_ass, feed_dict={mlp_model.emb_new: embeddings})\n",
    "        #train...\n",
    "        train_epochs(sess, epochs[idx], lrs[idx], bss[idx], True, parts_, w=None, learning_rate_decay=False, checkpoint_id=F_train_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train or Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F_num_epochs = 100\n",
    "F_is_train, F_reuse_model, F_train_version, F_inference_version = 0, 0, 0, 2\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
    "    if(F_is_train):\n",
    "        if(F_train_version == 0):\n",
    "            restore_model(mlp_model, sess, F_train_version)\n",
    "            lr_op = mlp_model.learning_rate.assign(F_learning_rate)\n",
    "            sess.run(lr_op)\n",
    "        else:\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "        train_epochs(sess, F_num_epochs, F_learning_rate, F_batch_size, False, checkpoint_id=F_train_version if F_reuse_model else 0)\n",
    "    else:\n",
    "        load_emb(mlp_model, sess, F_inference_version)\n",
    "        \n",
    "        idxs_test, y_test = idxs_val, y_val\n",
    "        pred, avr = inference(mlp_model, sess, idxs_test)\n",
    "        diff_ = np.abs(pred - y_test)\n",
    "        diff_rel = diff_ / np.maximum(y_test, 1)\n",
    "        print(\"    abs:\", np.mean(diff_))\n",
    "        print(\"    rel:\", np.mean(diff_rel))\n",
    "        print(\"rel std:\", np.std(diff_rel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
